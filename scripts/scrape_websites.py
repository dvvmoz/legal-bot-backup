#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ –∏ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
"""

import sys
import os
import argparse
import logging
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from modules.web_scraper import WebScraper, create_scraper_from_config
from modules.knowledge_base import KnowledgeBase
from modules.text_processing import TextProcessor


def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/scraping.log'),
            logging.StreamHandler()
        ]
    )


def scrape_single_site(url: str, max_pages: int = 20):
    """
    –°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞
    
    Args:
        url: URL —Å–∞–π—Ç–∞ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å–∞–π—Ç–∞: {url}")
    
    scraper = create_scraper_from_config()
    result = scraper.scrape_and_add(url, max_pages)
    
    if result['success']:
        print(f"‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {result['pages_scraped']}")
        print(f"üìù –î–æ–±–∞–≤–ª–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {result['chunks_added']}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {result['message']}")
    
    return result


def scrape_multiple_sites(urls: list, max_pages_per_site: int = 10):
    """
    –°–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
    
    Args:
        urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        max_pages_per_site: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ —Å–∞–π—Ç
    """
    print(f"üåê –ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ {len(urls)} —Å–∞–π—Ç–æ–≤")
    
    scraper = create_scraper_from_config()
    total_pages = 0
    total_chunks = 0
    
    for i, url in enumerate(urls, 1):
        print(f"\nüìã –°–∞–π—Ç {i}/{len(urls)}: {url}")
        
        result = scraper.scrape_and_add(url, max_pages_per_site)
        
        if result['success']:
            total_pages += result['pages_scraped']
            total_chunks += result['chunks_added']
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {result['pages_scraped']} —Å—Ç—Ä–∞–Ω–∏—Ü, {result['chunks_added']} —á–∞–Ω–∫–æ–≤")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['message']}")
    
    print(f"\nüéâ –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
    print(f"üìù –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")


def get_legal_sites_list():
    """
    –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ (–†–ë + –†–§)
    
    Returns:
        –°–ø–∏—Å–æ–∫ URL —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
    """
    return [
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ—Ä—Ç–∞–ª—ã –ë–µ–ª–∞—Ä—É—Å–∏
        "https://pravo.by/",
        "https://www.government.by/",
        "https://www.house.gov.by/",
        "https://www.kc.gov.by/",
        "https://www.court.gov.by/",
        "https://www.prokuratura.gov.by/",
        "https://www.minjust.gov.by/",
        "https://www.notariat.by/",
        "https://www.lawbelarus.com/",
        "https://www.jurist.by/",
        
        # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ—Ä—Ç–∞–ª—ã (–¥–ª—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
        "https://www.garant.ru/",
        "https://www.consultant.ru/",
        "https://www.pravo.gov.ru/",
        "https://www.advgazeta.ru/",
        "https://www.law.ru/"
    ]


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ –∏ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"
    )
    
    parser.add_argument(
        '--url',
        type=str,
        help='URL —Å–∞–π—Ç–∞ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞'
    )
    
    parser.add_argument(
        '--max-pages',
        type=int,
        default=20,
        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 20)'
    )
    
    parser.add_argument(
        '--sites-file',
        type=str,
        help='–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ (–ø–æ –æ–¥–Ω–æ–º—É URL –Ω–∞ —Å—Ç—Ä–æ–∫—É)'
    )
    
    parser.add_argument(
        '--demo',
        action='store_true',
        help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–µ–º–æ-—Å–∫—Ä–∞–ø–∏–Ω–≥ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤'
    )
    
    parser.add_argument(
        '--list-sites',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤'
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging()
    
    if args.list_sites:
        print("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å–∞–π—Ç—ã –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞:")
        sites = get_legal_sites_list()
        for i, site in enumerate(sites, 1):
            print(f"{i:2d}. {site}")
        return
    
    if args.demo:
        print("üéØ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ-—Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤")
        sites = get_legal_sites_list()[:3]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å–∞–π—Ç–∞ –¥–ª—è –¥–µ–º–æ
        scrape_multiple_sites(sites, max_pages_per_site=5)
        return
    
    if args.sites_file:
        if not os.path.exists(args.sites_file):
            print(f"‚ùå –§–∞–π–ª {args.sites_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        with open(args.sites_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        
        if not urls:
            print("‚ùå –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö URL")
            return
        
        scrape_multiple_sites(urls, args.max_pages)
        return
    
    if args.url:
        scrape_single_site(args.url, args.max_pages)
        return
    
    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ø—Ä–∞–≤–∫—É
    print("üîç –°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤")
    print("\nüìñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
    print("  python scripts/scrape_websites.py --url https://example.com")
    print("  python scripts/scrape_websites.py --demo")
    print("  python scripts/scrape_websites.py --sites-file urls.txt")
    print("  python scripts/scrape_websites.py --list-sites")
    print("\nüí° –ü—Ä–∏–º–µ—Ä—ã:")
    print("  # –°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞")
    print("  python scripts/scrape_websites.py --url https://www.garant.ru/ --max-pages 10")
    print("\n  # –î–µ–º–æ-—Å–∫—Ä–∞–ø–∏–Ω–≥ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å–∞–π—Ç–æ–≤")
    print("  python scripts/scrape_websites.py --demo")
    print("\n  # –°–∫—Ä–∞–ø–∏–Ω–≥ –∏–∑ —Ñ–∞–π–ª–∞")
    print("  python scripts/scrape_websites.py --sites-file legal_sites.txt")


if __name__ == "__main__":
    main() 