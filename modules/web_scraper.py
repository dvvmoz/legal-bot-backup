"""
–ú–æ–¥—É–ª—å –¥–ª—è –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ –∏ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import logging
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Set, Optional
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import json
import os

from .text_processing import TextProcessor
from .knowledge_base import KnowledgeBase
from .legal_content_filter import create_legal_content_filter

logger = logging.getLogger(__name__)


class WebScraper:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""
    
    def __init__(self, knowledge_base: KnowledgeBase, text_processor: TextProcessor):
        self.knowledge_base = knowledge_base
        self.text_processor = text_processor
        self.legal_filter = create_legal_content_filter()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.visited_urls: Set[str] = set()
        self.max_pages = 50  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        self.delay = 1  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        
    def scrape_single_page(self, url: str) -> Optional[Dict]:
        """
        –°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            logger.info(f"–°–∫—Ä–∞–ø–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content = ""
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–≥–∞—Ö
            main_content_selectors = [
                'main', 'article', '.content', '.main-content', 
                '.post-content', '.entry-content', '#content', '#main'
            ]
            
            for selector in main_content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, –±–µ—Ä–µ–º –≤–µ—Å—å body
            if not content:
                body = soup.find('body')
                if body:
                    content = body.get_text(separator=' ', strip=True)
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            content = self._clean_text(content)
            
            if len(content) < 100:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
                return None
            
            return {
                'url': url,
                'title': title_text,
                'content': content,
                'domain': urlparse(url).netloc
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ {url}: {e}")
            return None
    
    def _clean_text(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)\[\]]', '', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        text = text.strip()
        
        return text
    
    def get_legal_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            base_url: –ë–∞–∑–æ–≤—ã–π URL
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        """
        links = []
        domain = urlparse(base_url).netloc
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–†–ë + –†–§)
        legal_keywords = [
            # –û–±—â–∏–µ –ø—Ä–∞–≤–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            '–∑–∞–∫–æ–Ω', '–∫–æ–¥–µ–∫—Å', '–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ', '—É–∫–∞–∑', '–ø—Ä–∏–∫–∞–∑',
            '—Ä–µ–≥–ª–∞–º–µ–Ω—Ç', '–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–º–µ—Ç–æ–¥–∏–∫–∞',
            '–ø—Ä–∞–≤–æ', '—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π', '–ø—Ä–∞–≤–æ–≤–æ–π', '–∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ',
            '—Å—É–¥', '–∞–¥–≤–æ–∫–∞—Ç', '–Ω–æ—Ç–∞—Ä–∏—É—Å', '–¥–æ–≥–æ–≤–æ—Ä', '–∏—Å–∫',
            '–∑–∞—è–≤–ª–µ–Ω–∏–µ', '–∂–∞–ª–æ–±–∞', '–∞–ø–µ–ª–ª—è—Ü–∏—è', '–∫–∞—Å—Å–∞—Ü–∏—è',
            
            # –°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –¥–ª—è –ë–µ–ª–∞—Ä—É—Å–∏
            '—Ä–µ—Å–ø—É–±–ª–∏–∫–∞ –±–µ–ª–∞—Ä—É—Å—å', '–±–µ–ª–∞—Ä—É—Å—å', '–±–µ–ª–æ—Ä—É—Å—Å–∫–∏–π',
            '—Å–æ–≤–µ—Ç –º–∏–Ω–∏—Å—Ç—Ä–æ–≤', '–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–±—Ä–∞–Ω–∏–µ', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç',
            '–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–π —Å—É–¥', '–≤–µ—Ä—Ö–æ–≤–Ω—ã–π —Å—É–¥', '—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—É–¥',
            '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞', '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —é—Å—Ç–∏—Ü–∏–∏', '–Ω–æ—Ç–∞—Ä–∏–∞—Ç',
            '–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç', '–æ–±–ª–∏—Å–ø–æ–ª–∫–æ–º', '–≥–æ—Ä–∏—Å–ø–æ–ª–∫–æ–º',
            '—Ç—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å', '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å', '—É–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å',
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å', '–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å',
            '–¥–µ–∫—Ä–µ—Ç', '—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ', '—Ä–µ—à–µ–Ω–∏–µ', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ'
        ]
        
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            link_text = link.get_text().lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –≤–µ–¥–µ—Ç –Ω–∞ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
            full_url = urljoin(base_url, href)
            if urlparse(full_url).netloc != domain:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–∫–∏
            if any(keyword in link_text for keyword in legal_keywords):
                links.append(full_url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL
            if any(keyword in href.lower() for keyword in legal_keywords):
                links.append(full_url)
        
        return list(set(links))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def scrape_website(self, start_url: str, max_pages: int = None) -> List[Dict]:
        """
        –°–∫—Ä–∞–ø–∏–Ω–≥ –≤—Å–µ–≥–æ —Å–∞–π—Ç–∞ –Ω–∞—á–∏–Ω–∞—è —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ URL
        
        Args:
            start_url: –ù–∞—á–∞–ª—å–Ω—ã–π URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
        """
        if max_pages is None:
            max_pages = self.max_pages
            
        pages_data = []
        urls_to_visit = [start_url]
        self.visited_urls.clear()
        
        page_count = 0
        
        while urls_to_visit and page_count < max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in self.visited_urls:
                continue
                
            self.visited_urls.add(current_url)
            
            # –°–∫—Ä–∞–ø–∏–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            page_data = self.scrape_single_page(current_url)
            
            if page_data:
                pages_data.append(page_data)
                page_count += 1
                
                # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø–æ—Å–µ—â–µ–Ω–∏—è
                try:
                    response = self.session.get(current_url, timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    new_links = self.get_legal_links(soup, current_url)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
                    for link in new_links:
                        if link not in self.visited_urls and link not in urls_to_visit:
                            urls_to_visit.append(link)
                            
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ —Å {current_url}: {e}")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(self.delay)
        
        logger.info(f"–°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages_data)}")
        return pages_data
    
    async def scrape_website_async(self, start_url: str, max_pages: int = None) -> List[Dict]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å–∞–π—Ç–∞
        
        Args:
            start_url: –ù–∞—á–∞–ª—å–Ω—ã–π URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
        """
        if max_pages is None:
            max_pages = self.max_pages
            
        pages_data = []
        urls_to_visit = [start_url]
        self.visited_urls.clear()
        
        async with aiohttp.ClientSession() as session:
            page_count = 0
            
            while urls_to_visit and page_count < max_pages:
                current_url = urls_to_visit.pop(0)
                
                if current_url in self.visited_urls:
                    continue
                    
                self.visited_urls.add(current_url)
                
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_data = await self._scrape_page_async(session, current_url)
                
                if page_data:
                    pages_data.append(page_data)
                    page_count += 1
                    
                    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏
                    new_links = await self._get_links_async(session, current_url)
                    
                    for link in new_links:
                        if link not in self.visited_urls and link not in urls_to_visit:
                            urls_to_visit.append(link)
                
                await asyncio.sleep(self.delay)
        
        logger.info(f"–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages_data)}")
        return pages_data
    
    async def _scrape_page_async(self, session: aiohttp.ClientSession, url: str) -> Optional[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    return None
                    
                content = await response.text()
                soup = BeautifulSoup(content, 'html.parser')
                
                # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                    element.decompose()
                
                title = soup.find('title')
                title_text = title.get_text().strip() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                
                content_text = soup.get_text(separator=' ', strip=True)
                content_text = self._clean_text(content_text)
                
                if len(content_text) < 100:
                    return None
                
                return {
                    'url': url,
                    'title': title_text,
                    'content': content_text,
                    'domain': urlparse(url).netloc
                }
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º —Å–∫—Ä–∞–ø–∏–Ω–≥–µ {url}: {e}")
            return None
    
    async def _get_links_async(self, session: aiohttp.ClientSession, url: str) -> List[str]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    return []
                    
                content = await response.text()
                soup = BeautifulSoup(content, 'html.parser')
                return self.get_legal_links(soup, url)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ —Å {url}: {e}")
            return []
    
    def add_to_knowledge_base(self, pages_data: List[Dict]) -> int:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            pages_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not pages_data:
            return 0
        
        # –°–Ω–∞—á–∞–ª–∞ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        logger.info(f"üîç WEB_SCRAPER: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è {len(pages_data)} —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å")
        filtered_pages = self.legal_filter.filter_scraped_content(pages_data)
        
        if not filtered_pages:
            logger.info("üö´ WEB_SCRAPER: –ù–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
            return 0
        
        logger.info(f"‚úÖ WEB_SCRAPER: {len(filtered_pages)} –∏–∑ {len(pages_data)} —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä")
        
        added_count = 0
        
        for page_data in filtered_pages:
            try:
                # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏
                chunks = self.text_processor.split_text(page_data['content'])
                
                for i, chunk in enumerate(chunks):
                    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —á–∞–Ω–∫–∞ –∏–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "dynamic_" –∏ timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    url_hash = hash(page_data['url']) % 1000000  # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ö–µ—à
                    doc_id = f"dynamic_{timestamp}_{url_hash}_chunk_{i:03d}"
                    
                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞
                    metadata = {
                        'source': 'web_scraper',
                        'url': page_data['url'],
                        'title': page_data['title'],
                        'domain': page_data['domain'],
                        'chunk_index': i,
                        'total_chunks': len(chunks),
                        'content_type': 'legal_website',
                        'source_type': 'pravo.by_dynamic',
                        'scraped_at': timestamp,
                        'legal_score': page_data.get('legal_score', 0.0),
                        'legal_explanation': page_data.get('legal_explanation', ''),
                        'filtered_at': page_data.get('filtered_at', '')
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    if self.knowledge_base.add_document(doc_id, chunk, metadata):
                        added_count += 1
                        logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ {doc_id} –∏–∑ {page_data['url']}")
                    else:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ {doc_id}")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_data['url']}: {e}")
        
        logger.info(f"üíæ WEB_SCRAPER: –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {added_count} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(filtered_pages)} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
        return added_count
    
    def scrape_and_add(self, start_url: str, max_pages: int = None) -> Dict:
        """
        –°–∫—Ä–∞–ø–∏–Ω–≥ —Å–∞–π—Ç–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        
        Args:
            start_url: –ù–∞—á–∞–ª—å–Ω—ã–π URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å–∞–π—Ç–∞: {start_url}")
        
        # –°–∫—Ä–∞–ø–∏–º —Å–∞–π—Ç
        pages_data = self.scrape_website(start_url, max_pages)
        
        if not pages_data:
            return {
                'success': False,
                'message': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–∞',
                'pages_scraped': 0,
                'chunks_added': 0
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        chunks_added = self.add_to_knowledge_base(pages_data)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä—Å–∏–Ω–≥–µ
        try:
            from .scraping_tracker import update_scraping_info
            update_scraping_info(start_url, len(pages_data), chunks_added)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
        
        return {
            'success': True,
            'message': f'–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(pages_data)} —Å—Ç—Ä–∞–Ω–∏—Ü',
            'pages_scraped': len(pages_data),
            'chunks_added': chunks_added,
            'start_url': start_url
        }


def create_scraper_from_config() -> WebScraper:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ WebScraper —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    
    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä WebScraper
    """
    from .knowledge_base import KnowledgeBase
    from .text_processing import TextProcessor
    
    knowledge_base = KnowledgeBase()
    text_processor = TextProcessor()
    
    return WebScraper(knowledge_base, text_processor)


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    scraper = create_scraper_from_config()
    
    # –°–∫—Ä–∞–ø–∏–Ω–≥ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞
    result = scraper.scrape_and_add(
        start_url="https://www.garant.ru/",
        max_pages=10
    )
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {result}") 