"""
–ú–æ–¥—É–ª—å –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ pravo.by
–∫–æ–≥–¥–∞ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
"""
import requests
from bs4 import BeautifulSoup
import re
import logging
from urllib.parse import urljoin, quote
from typing import List, Dict, Optional, Tuple
import time
from datetime import datetime

from .web_scraper import WebScraper
from .knowledge_base import KnowledgeBase
from .text_processing import TextProcessor
from .scraping_tracker import ScrapingTracker
from .legal_content_filter import create_legal_content_filter

logger = logging.getLogger(__name__)

class DynamicSearcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ pravo.by"""
    
    def __init__(self, web_scraper: WebScraper, knowledge_base: KnowledgeBase, 
                 text_processor: TextProcessor, scraping_tracker: ScrapingTracker):
        self.web_scraper = web_scraper
        self.knowledge_base = knowledge_base
        self.text_processor = text_processor
        self.scraping_tracker = scraping_tracker
        self.legal_filter = create_legal_content_filter()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
        self.search_base_url = "https://pravo.by"
        self.search_endpoints = [
            "/search/",  # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫
            "/pravovaya-informatsiya/",  # –ü—Ä–∞–≤–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            "/natsionalnyy-reestr/",  # –ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä
        ]
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.max_search_results = 5
        self.max_pages_per_result = 3
        
    def _generate_search_queries(self, user_question: str) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        queries = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å
        queries.append(user_question)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(user_question)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if len(keywords) > 1:
            queries.append(" ".join(keywords[:3]))  # –ü–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
            
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –†–ë —Ç–µ—Ä–º–∏–Ω—ã
        rb_specific_terms = [
            "—Ä–µ—Å–ø—É–±–ª–∏–∫–∞ –±–µ–ª–∞—Ä—É—Å—å", "–±–µ–ª–∞—Ä—É—Å—å", "—Ä–±", "–±–µ–ª–æ—Ä—É—Å—Å–∫–∏–π",
            "–∑–∞–∫–æ–Ω", "–∫–æ–¥–µ–∫—Å", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ", "—É–∫–∞–∑", "–¥–µ–∫—Ä–µ—Ç"
        ]
        
        for term in rb_specific_terms:
            if term in user_question.lower():
                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å —ç—Ç–∏–º —Ç–µ—Ä–º–∏–Ω–æ–º
                term_query = f"{term} {' '.join(keywords[:2])}"
                if term_query not in queries:
                    queries.append(term_query)
                break
        
        return queries[:3]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        """
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫—Ç–æ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ',
            '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–ø—Ä–∏', '–∑–∞', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É',
            '–∏', '–∏–ª–∏', '–Ω–æ', '–∞', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–Ω–∏', '–∂–µ', '–ª–∏', '–±—ã', '—Ç–æ',
            '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–µ', '–º–æ–π', '–º–æ—è', '–º–æ–∏',
            '–µ–≥–æ', '–µ—ë', '–∏—Ö', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–∏', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–∏'
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ –æ—á–∏—â–∞–µ–º
        words = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        return keywords[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def _search_pravo_by(self, query: str) -> List[str]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –Ω–∞ pravo.by
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        """
        found_urls = []
        
        try:
            # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è URL
            encoded_query = quote(query)
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –ø–æ–∏—Å–∫–∞
            search_urls = [
                f"{self.search_base_url}/search/?q={encoded_query}",
                f"{self.search_base_url}/pravovaya-informatsiya/?search={encoded_query}",
            ]
            
            for search_url in search_urls:
                try:
                    logger.info(f"–ü–æ–∏—Å–∫ –ø–æ URL: {search_url}")
                    
                    response = self.web_scraper.session.get(search_url, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                    result_links = self._extract_search_results(soup, query)
                    
                    for link in result_links:
                        full_url = urljoin(self.search_base_url, link)
                        if full_url not in found_urls:
                            found_urls.append(full_url)
                    
                    time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ {search_url}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not found_urls:
                found_urls = self._find_relevant_pages(query)
            
            return found_urls[:self.max_search_results]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–∞ pravo.by: {e}")
            return []
    
    def _extract_search_results(self, soup: BeautifulSoup, query: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL
        """
        links = []
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        search_selectors = [
            'a[href*="/novosti/"]',
            'a[href*="/pravovaya-informatsiya/"]',
            'a[href*="/natsionalnyy-reestr/"]',
            'a[href*="/gosudarstvo-i-pravo/"]',
            '.search-result a',
            '.result-item a',
            '.content-item a'
        ]
        
        query_words = query.lower().split()
        
        for selector in search_selectors:
            elements = soup.select(selector)
            for element in elements:
                href = element.get('href')
                text = element.get_text().lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                if href and any(word in text for word in query_words):
                    links.append(href)
        
        return list(set(links))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _find_relevant_pages(self, query: str) -> List[str]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL
        """
        relevant_urls = []
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            topic_mappings = {
                '—Ç—Ä—É–¥–æ–≤–æ–π': '/pravovaya-informatsiya/trudovoe-pravo/',
                '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π': '/pravovaya-informatsiya/grazhdanskoe-pravo/',
                '—Å–µ–º–µ–π–Ω—ã–π': '/pravovaya-informatsiya/semeynoe-pravo/',
                '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π': '/pravovaya-informatsiya/administrativnoe-pravo/',
                '—É–≥–æ–ª–æ–≤–Ω—ã–π': '/pravovaya-informatsiya/ugolovnoe-pravo/',
                '—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω—ã–π': '/pravovaya-informatsiya/khozyaystvennoe-pravo/',
                '–Ω–∞–ª–æ–≥–æ–≤—ã–π': '/pravovaya-informatsiya/nalogovoe-pravo/',
                '–∑–µ–º–µ–ª—å–Ω—ã–π': '/pravovaya-informatsiya/zemelnoe-pravo/',
                '–∂–∏–ª–∏—â–Ω—ã–π': '/pravovaya-informatsiya/zhilishchnoe-pravo/',
                '–∏–ø': '/pravovaya-informatsiya/individualnoe-predprinimatelstvo/',
                '–æ–æ–æ': '/pravovaya-informatsiya/obshchestva-ogranichennoj-otvetstvennostyu/',
                '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è': '/pravovaya-informatsiya/registratsiya/',
                '—Ä–∞–∑–≤–æ–¥': '/pravovaya-informatsiya/semeynoe-pravo/',
                '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': '/pravovaya-informatsiya/trudovoe-pravo/',
                '–¥–æ–≥–æ–≤–æ—Ä': '/pravovaya-informatsiya/dogovornoe-pravo/',
                '–Ω–∞—Å–ª–µ–¥—Å—Ç–≤–æ': '/pravovaya-informatsiya/nasledstvennoe-pravo/',
                '–∞–ª–∏–º–µ–Ω—Ç—ã': '/pravovaya-informatsiya/semeynoe-pravo/',
                '—à—Ç—Ä–∞—Ñ': '/pravovaya-informatsiya/administrativnoe-pravo/',
                '—Å—É–¥': '/pravovaya-informatsiya/sudebnaya-sistema/',
                '–ø—Ä–∞–≤–∞': '/pravovaya-informatsiya/prava-grazhdan/',
                '–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏': '/pravovaya-informatsiya/obyazannosti-grazhdan/'
            }
            
            query_lower = query.lower()
            
            # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ä–∞–∑–¥–µ–ª—ã
            for keyword, url_path in topic_mappings.items():
                if keyword in query_lower:
                    full_url = urljoin(self.search_base_url, url_path)
                    relevant_urls.append(full_url)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã, –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ
            if not relevant_urls:
                general_urls = [
                    f"{self.search_base_url}/pravovaya-informatsiya/",
                    f"{self.search_base_url}/natsionalnyy-reestr/novye-postupleniya/",
                    f"{self.search_base_url}/novosti/analitika/"
                ]
                relevant_urls.extend(general_urls)
            
            return relevant_urls[:self.max_search_results]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {e}")
            return []
    
    def _check_if_info_already_exists(self, user_question: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –ø–æ—Ö–æ–∂–µ–º—É –∑–∞–ø—Ä–æ—Å—É –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.
        
        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            True –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —É–∂–µ –µ—Å—Ç—å, False –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        """
        try:
            from .knowledge_base import search_relevant_docs
            
            # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            relevant_docs = search_relevant_docs(user_question, n_results=5)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Ä–µ–¥–∏ –Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å pravo.by
            dynamic_docs = [
                doc for doc in relevant_docs 
                if doc.get('metadata', {}).get('source_type') == 'pravo.by_dynamic'
            ]
            
            if dynamic_docs:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                best_distance = min(doc['distance'] for doc in dynamic_docs)
                
                if best_distance < 0.6:  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ø–æ—Ä–æ–≥ —Å 0.4 –¥–æ 0.6 –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–æ–≥–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
                    logger.info(f"üîÑ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ù–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {best_distance:.3f}) - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à")
                    return True
                else:
                    logger.info(f"üîÑ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ (–¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {best_distance:.3f}) - –∏—â–µ–º –∑–∞–Ω–æ–≤–æ")
                    return False
            
            logger.info(f"üîÑ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–∏—Å–∫ –Ω–∞ pravo.by")
            return False
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–µ—à–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return False
    
    def search_and_add_to_knowledge_base(self, user_question: str) -> Tuple[Optional[str], bool]:
        """
        –ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ pravo.by –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        
        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            Tuple[–Ω–∞–π–¥–µ–Ω–Ω–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å_–æ–ø–µ—Ä–∞—Ü–∏–∏]
        """
        logger.info(f"üîç –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ó–∞–ø—Ä–æ—Å - {user_question}")
        
        try:
            # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞ —Ç–µ–ø–µ—Ä—å –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ bot_handler.py
            # –°—Ä–∞–∑—É –∏—â–µ–º –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ pravo.by
            logger.info(f"üåê –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ò—â–µ–º –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ pravo.by")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            search_queries = self._generate_search_queries(user_question)
            logger.info(f"üîç –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞–ø—Ä–æ—Å—ã: {search_queries}")
            
            all_found_urls = []
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É
            for query in search_queries:
                found_urls = self._search_pravo_by(query)
                all_found_urls.extend(found_urls)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_urls = list(set(all_found_urls))
            
            if not unique_urls:
                logger.info("üö´ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ pravo.by")
                return None, False
            
            logger.info(f"üéØ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ù–∞–π–¥–µ–Ω–æ {len(unique_urls)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            
            # –ü–∞—Ä—Å–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            scraped_data = []
            for url in unique_urls[:self.max_search_results]:
                try:
                    page_data = self.web_scraper.scrape_single_page(url)
                    if page_data and len(page_data['content']) > 200:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        scraped_data.append(page_data)
                        logger.info(f"üìÑ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                    
                    time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                    continue
            
            if not scraped_data:
                logger.info("üö´ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
                return None, False
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            logger.info(f"üîç –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è {len(scraped_data)} —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å")
            filtered_data = self.legal_filter.filter_scraped_content(scraped_data)
            
            if not filtered_data:
                logger.info("üö´ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ù–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
                return None, False
            
            logger.info(f"‚úÖ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: {len(filtered_data)} –∏–∑ {len(scraped_data)} —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Ç–æ–ª—å–∫–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            logger.info(f"üíæ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –î–æ–±–∞–≤–ª—è–µ–º {len(filtered_data)} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
            chunks_added = self.web_scraper.add_to_knowledge_base(filtered_data)
            
            if chunks_added > 0:
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä—Å–∏–Ω–≥–µ
                self.scraping_tracker.update_scraping_info(
                    "https://pravo.by/", 
                    len(scraped_data), 
                    chunks_added
                )
                
                logger.info(f"‚úÖ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –î–æ–±–∞–≤–ª–µ–Ω–æ {chunks_added} —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
                
                # –¢–µ–ø–µ—Ä—å –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
                from .knowledge_base import search_relevant_docs
                from .llm_service import get_answer
                
                relevant_docs = search_relevant_docs(user_question, n_results=5)
                
                if relevant_docs:
                    logger.info(f"ü§ñ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {len(scraped_data)} —Å—Ç—Ä–∞–Ω–∏—Ü")
                    answer = get_answer(user_question, relevant_docs)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ
                    source_info = f"\n\nüìç –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∏–∑ {len(scraped_data)} —Å—Ç—Ä–∞–Ω–∏—Ü pravo.by"
                    answer += source_info
                    
                    logger.info(f"‚úÖ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –û—Ç–≤–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                    return answer, True
                else:
                    logger.info(f"üö´ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–∞–∂–µ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                    return None, False
            else:
                logger.info("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
                return None, False
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return None, False
    
    def get_search_statistics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        return {
            "search_base_url": self.search_base_url,
            "max_search_results": self.max_search_results,
            "max_pages_per_result": self.max_pages_per_result,
            "available_endpoints": self.search_endpoints
        }


def create_dynamic_searcher(web_scraper: WebScraper, knowledge_base: KnowledgeBase, 
                          text_processor: TextProcessor, scraping_tracker: ScrapingTracker) -> DynamicSearcher:
    """–°–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞"""
    return DynamicSearcher(web_scraper, knowledge_base, text_processor, scraping_tracker) 